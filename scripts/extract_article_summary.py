#!/usr/bin/env python3
"""
HelloAI æ–‡ç« æ‘˜è¦æå–è„šæœ¬

ç”¨äºæå–æ–‡ç« çš„å…ƒä¿¡æ¯å’Œæ‘˜è¦ï¼Œæ”¯æŒä» meta.json è¯»å–è¯¦ç»†ä¿¡æ¯ã€‚
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional


def extract_article_summary(article_path: str) -> Optional[Dict]:
    """æå–å•ä¸ªæ–‡ç« çš„æ‘˜è¦ä¿¡æ¯"""
    article_path = Path(article_path)
    
    if not article_path.exists():
        print(f"âŒ æ–‡ç« ä¸å­˜åœ¨: {article_path}")
        return None
    
    try:
        # è¯»å–æ–‡ç« å†…å®¹
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        article_dir = article_path.parent
        meta_path = article_dir / 'meta.json'
        
        # è¯»å–å…ƒä¿¡æ¯
        meta_info = {}
        if meta_path.exists():
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta_info = json.load(f)
                print(f"  ğŸ“„ è¯»å–å…ƒä¿¡æ¯: {article_dir.name}")
            except Exception as e:
                print(f"  âš ï¸ å…ƒä¿¡æ¯è¯»å–å¤±è´¥: {e}")
        
        # æå–åŸºç¡€ä¿¡æ¯
        lines = content.split('\n')
        title = None
        summary_lines = []
        
        # æ‰¾åˆ°æ ‡é¢˜
        for line in lines:
            if line.startswith('# '):
                title = line[2:].strip()
                break
        
        # æå–å¼€åœºç™½ä½œä¸ºæ‘˜è¦ï¼ˆä»å¼•è¨€æˆ–ç¬¬ä¸€æ®µæ­£æ–‡ï¼‰
        in_content = False
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡æ ‡é¢˜å’Œç©ºè¡Œ
            if line.startswith('#') or not line:
                continue
            
            # æ‰¾åˆ°å¼€åœºç™½æ®µè½
            if line.startswith('>'):
                continue
            elif line and not line.startswith('##') and not line.startswith('!['):
                summary_lines.append(line)
                if len(summary_lines) >= 3:  # å–å‰3è¡Œä½œä¸ºæ‘˜è¦
                    break
        
        # ç»„åˆæ‘˜è¦
        summary = ' '.join(summary_lines)[:200] + '...' if summary_lines else ''
        
        # ç»Ÿè®¡ä¿¡æ¯
        images_dir = article_dir / 'images'
        images_count = len(list(images_dir.glob('*'))) if images_dir.exists() else 0
        
        # æ„å»ºæ‘˜è¦ä¿¡æ¯
        summary_info = {
            'path': str(article_path),
            'dir_name': article_dir.name,
            'title': title or 'æœªçŸ¥æ ‡é¢˜',
            'summary': summary,
            'content_length': len(content),
            'images_count': images_count,
            'has_cover': (article_dir / 'cover.jpg').exists(),
            'has_meta': meta_path.exists(),
            'extracted_at': datetime.now().isoformat()
        }
        
        # æ·»åŠ å…ƒä¿¡æ¯å­—æ®µ
        if meta_info:
            summary_info.update({
                'title_hook': meta_info.get('title_hook'),
                'description': meta_info.get('article_description'),
                'keywords': meta_info.get('keywords', []),
                'cover_copy_text': meta_info.get('cover_copy_text'),
                'product_count': meta_info.get('product_count', 0),
                'issue_number': meta_info.get('issue_info', {}).get('issue_number'),
                'publish_date': meta_info.get('publish_date'),
                'generated_at': meta_info.get('generated_at')
            })
        
        return summary_info
        
    except Exception as e:
        print(f"âŒ æå–æ‘˜è¦å¤±è´¥: {article_path}, é”™è¯¯: {e}")
        return None


def extract_all_articles_summary(articles_dir: str = 'articles') -> List[Dict]:
    """æå–æ‰€æœ‰æ–‡ç« çš„æ‘˜è¦ä¿¡æ¯"""
    articles_path = Path(articles_dir)
    if not articles_path.exists():
        print(f"âŒ æ–‡ç« ç›®å½•ä¸å­˜åœ¨: {articles_path}")
        return []
    
    all_summaries = []
    
    # éå†æ‰€æœ‰æ–‡ç« ç›®å½•
    for year_dir in sorted(articles_path.iterdir()):
        if not year_dir.is_dir():
            continue
            
        for date_dir in sorted(year_dir.iterdir()):
            if not date_dir.is_dir():
                continue
                
            index_file = date_dir / 'index.md'
            if index_file.exists():
                summary = extract_article_summary(str(index_file))
                if summary:
                    all_summaries.append(summary)
                    print(f"  âœ… {summary['title'][:50]}...")
    
    return all_summaries


def save_summaries_to_file(summaries: List[Dict], output_path: str = 'config/articles_summary.json'):
    """ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary_data = {
        'generated_at': datetime.now().isoformat(),
        'total_articles': len(summaries),
        'articles': summaries
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    return output_path


def print_summary_stats(summaries: List[Dict]):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    if not summaries:
        print("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
        return
    
    total = len(summaries)
    with_meta = len([s for s in summaries if s.get('has_meta')])
    with_cover = len([s for s in summaries if s.get('has_cover')])
    total_images = sum(s.get('images_count', 0) for s in summaries)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  ğŸ“° æ€»æ–‡ç« æ•°: {total}")
    print(f"  ğŸ“„ æœ‰å…ƒä¿¡æ¯: {with_meta} ({with_meta/total*100:.1f}%)")
    print(f"  ğŸ¨ æœ‰å°é¢å›¾: {with_cover} ({with_cover/total*100:.1f}%)")
    print(f"  ğŸ–¼ï¸  æ€»å›¾ç‰‡æ•°: {total_images}")
    
    # æœ€è¿‘çš„æ–‡ç« 
    recent = sorted(summaries, key=lambda x: x.get('publish_date', ''), reverse=True)[:3]
    print(f"\nğŸ“… æœ€è¿‘æ–‡ç« :")
    for article in recent:
        date = article.get('publish_date', 'æœªçŸ¥æ—¥æœŸ')
        print(f"  - {date}: {article['title'][:40]}...")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='HelloAI æ–‡ç« æ‘˜è¦æå–å·¥å…·')
    parser.add_argument('--single', '-s', help='æå–å•ä¸ªæ–‡ç« æ‘˜è¦', metavar='PATH')
    parser.add_argument('--all', '-a', action='store_true', help='æå–æ‰€æœ‰æ–‡ç« æ‘˜è¦')
    parser.add_argument('--output', '-o', default='config/articles_summary.json', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--articles-dir', default='articles', help='æ–‡ç« ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.single:
        print(f"ğŸ” æå–å•ä¸ªæ–‡ç« æ‘˜è¦: {args.single}")
        summary = extract_article_summary(args.single)
        if summary:
            print(f"âœ… æå–æˆåŠŸ:")
            print(json.dumps(summary, ensure_ascii=False, indent=2))
        
    elif args.all:
        print(f"ğŸ” æå–æ‰€æœ‰æ–‡ç« æ‘˜è¦...")
        summaries = extract_all_articles_summary(args.articles_dir)
        
        if summaries:
            output_file = save_summaries_to_file(summaries, args.output)
            print_summary_stats(summaries)
            print(f"\nğŸ‰ å®Œæˆ! æå–äº† {len(summaries)} ç¯‡æ–‡ç« çš„æ‘˜è¦")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()